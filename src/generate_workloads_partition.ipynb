{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_to_partition = \"/home/lenovo/Downloads/graph-for-metis.txt.part.8\"\n",
    "num_partitions = int(path_to_partition.split('.')[-1])\n",
    "\n",
    "configs_dir = f\"final_topology_0\"\n",
    "topology_json = \"reindexed\"\n",
    "\n",
    "B = 125_000_000 # BYTES\n",
    "\n",
    "'''\n",
    "ZTE         - Generates a workload for the ZTE topoloy\n",
    "FAT_TREE    - Generates a workload for a fat tree topology\n",
    "'''\n",
    "topology = 'ZTE'\n",
    "\n",
    "out_name = f'traces/trace_{configs_dir}'                          # name for saved trace\n",
    "config_path = os.getcwd() + f'/topologies/{configs_dir}/'         # path to switch config files for topology\n",
    "data_path = f'data/{topology_json}.json'                          # path to file containing topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 0 -> src: 621 dst: 12\n",
      "partition: 1 -> src: 657 dst: 9\n",
      "partition: 2 -> src: 635 dst: 17\n",
      "partition: 3 -> src: 657 dst: 12\n",
      "partition: 4 -> src: 641 dst: 16\n",
      "partition: 5 -> src: 649 dst: 5\n",
      "partition: 6 -> src: 647 dst: 4\n",
      "partition: 7 -> src: 642 dst: 13\n"
     ]
    }
   ],
   "source": [
    "configs = os.listdir(config_path)\n",
    "\n",
    "with open(data_path,'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "nodes = {}\n",
    "for node in data['nodeList']:\n",
    "    nodes[node['id']] = node\n",
    "\n",
    "with open(path_to_partition, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        node_id, partition = idx, line.strip()\n",
    "        nodes[node_id]['partition'] = int(partition)\n",
    "\n",
    "sources = {i:list() for i in range(num_partitions)} # access\n",
    "destinations = {i:list() for i in range(num_partitions)} # kernel or mixed \n",
    "\n",
    "match(topology):\n",
    "    case \"ZTE\":\n",
    "        source_name = 'Access'\n",
    "    case \"FAT_TREE\":\n",
    "        source_name = 'access'\n",
    "\n",
    "for config in configs:\n",
    "    id = int(config.split('.')[0])\n",
    "\n",
    "    if nodes[id]['deviceLevel'] == source_name:\n",
    "        sources[nodes[id]['partition']].append(id)\n",
    "    else:\n",
    "        destinations[nodes[id]['partition']].append(id)\n",
    "\n",
    "for (p_id, (p_src, p_dest)) in enumerate(zip(list(sources.values()), list(destinations.values()))):\n",
    "    print('partition:',p_id, '-> src:',len(p_src), 'dst:',len(p_dest))\n",
    "    assert(p_dest != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_THROUGHPUT = B                     # BYTES PER SECOND\n",
    "SIMULATION_TIME = 1000000               # Ns\n",
    "PAIRS_PER_SRC = {'mu': 4, 'sigma': 0}   # NORMAL DIST\n",
    "MSG_SIZE = 50_000                       # BYTES\n",
    "PACKET_SIZE = 1400                      # BYTES\n",
    "BANDWIDTH = 10_000_000                  # BYTES\n",
    "PRIO_LEVELS = 3                         # QOS PRIORITIES\n",
    "\n",
    "PERCENT_INTRA_PARTITION = .66\n",
    "PRCENT_INTER_PARTITION = 1 - PERCENT_INTRA_PARTITION\n",
    "\n",
    "s_to_ns = lambda x : int(x * math.pow(10,9))\n",
    "ns_to_s = lambda x : x * math.pow(10,-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOW_THROUGHPUT:125000000__SIMULATION_TIME:1000000__PAIRS_PER_SRC:(4, 0)__MSG_SIZE:50000__PACKET_SIZE:1400__BANDWIDTH:10000000__PRIO_LEVELS:3__INTRA:0.66__INTER:0.33999999999999997\n"
     ]
    }
   ],
   "source": [
    "parameters = (f\"FLOW_THROUGHPUT:{FLOW_THROUGHPUT}__\"\n",
    "          f\"SIMULATION_TIME:{SIMULATION_TIME}__\"\n",
    "          f\"PAIRS_PER_SRC:{tuple(PAIRS_PER_SRC.values())}__\"\n",
    "          f\"MSG_SIZE:{MSG_SIZE}__\"\n",
    "          f\"PACKET_SIZE:{PACKET_SIZE}__\"\n",
    "          f\"BANDWIDTH:{BANDWIDTH}__\"\n",
    "          f\"PRIO_LEVELS:{PRIO_LEVELS}__\"\n",
    "          f\"INTRA:{PERCENT_INTRA_PARTITION}__INTER:{PRCENT_INTER_PARTITION}\")\n",
    "\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate flows\n",
    "def generate_flows(per_flow=False):\n",
    "    flows = {}\n",
    "    # iterate over all srcs and dests in all partitions\n",
    "    for p_id_s, p_id_d, srcs, intra_dsts in zip(sources.keys(), destinations.keys(), sources.values(), destinations.values()):\n",
    "        assert(p_id_s == p_id_d) # ensure dicts are alligned (python now respects order of dicts but you never know)\n",
    "        p_id = p_id_s  \n",
    "        \n",
    "        # calculate inter dests i.e., destinations not in that partition\n",
    "        inter_dsts = []\n",
    "        for key, value in destinations.items():\n",
    "            if key != p_id:\n",
    "                inter_dsts += value\n",
    "\n",
    "        if per_flow:\n",
    "            # for eacn src in the current partition: calculate the number of dests and assing inter-intra dests based on the defined parameters\n",
    "            for src in srcs:\n",
    "                num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "                while num_pairs <= 0:\n",
    "                    num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "                # calculate percentages\n",
    "                num_inter, num_intra = round(num_pairs*PRCENT_INTER_PARTITION), round(num_pairs*PERCENT_INTRA_PARTITION)\n",
    "                # generate flows\n",
    "                flows[src] = [('intra', x) for x in random.sample(intra_dsts, k=num_intra)] + \\\n",
    "                            [('inter', x) for x in random.sample(inter_dsts, k=num_inter)]\n",
    "        else: # per partition\n",
    "            # calculate how many srcs will send intra and inter messages\n",
    "            num_inter, num_intra = round(len(srcs) * PRCENT_INTER_PARTITION), round(len(srcs)*PERCENT_INTRA_PARTITION)\n",
    "            inter_srcs = random.sample(srcs, k=num_inter)\n",
    "            intra_srcs = [x for x in srcs if x not in inter_srcs]\n",
    "\n",
    "            # randomly pick inter destinations\n",
    "            for src in inter_srcs:\n",
    "                num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "                while num_pairs <= 0:\n",
    "                    num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "        \n",
    "                flows[src] = random.sample(inter_dsts, k=num_pairs)\n",
    "                \n",
    "            # radomly pick inter destications\n",
    "            for src in intra_srcs:\n",
    "                num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "                while num_pairs <= 0:\n",
    "                    num_pairs = int(random.normalvariate(**PAIRS_PER_SRC))\n",
    "        \n",
    "                flows[src] = random.sample(intra_dsts, k=num_pairs)\n",
    "        \n",
    "    return flows\n",
    "\n",
    "flows = generate_flows()\n",
    "\n",
    "# # This is to make sure the currect prc of inter-intra flows is generated\n",
    "# eval = {key:{\n",
    "#             \"num_intra\": [x[0] for x in value].count('intra'),\n",
    "#             \"num_inter\":[x[0] for x in value].count('inter')\n",
    "#         }for key, value in flows.items()}\n",
    "\n",
    "# print('num_pairs:', PAIRS_PER_SRC, 'prc_intra:', PERCENT_INTRA_PARTITION, 'prc inter:', PRCENT_INTER_PARTITION)\n",
    "# print(\"flows for validation:\", eval)\n",
    "\n",
    "# # This removes the data requires for the validation to make the flows in the expected format\n",
    "# flows = {key:[x[1] for x in value] for key, value in flows.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_messages_for_flow(duration_ns):\n",
    "    total_bytes_for_duration = ns_to_s(duration_ns) * FLOW_THROUGHPUT\n",
    "\n",
    "    sizes = []\n",
    "\n",
    "    # generate message sizes using poisson distribution\n",
    "    while sum(sizes) < total_bytes_for_duration:\n",
    "        sizes.append(int(random.expovariate(1/MSG_SIZE)))\n",
    "    \n",
    "    # distribute the message send times uniformly over the simulation time\n",
    "    times = np.linspace(0, duration_ns, len(sizes))\n",
    "    times = [int(x) for x in times]\n",
    "\n",
    "    return times, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5149/5149 [00:01<00:00, 4006.85it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Generation Logic:\n",
    "        - For each source - while the current message time is less than the simulation time\n",
    "            - pick the next message time (exp dist)\n",
    "            - pick a random dest (uniform)\n",
    "            - pick a random message (size)\n",
    "            - split the message into packets based on packets size\n",
    "            - add packets accumulating transmission time (terminal -> switch)\n",
    "'''\n",
    "\n",
    "message_id = 0\n",
    "messages = []\n",
    "for src, pairs in tqdm(flows.items()):\n",
    "     for dst in pairs:\n",
    "        flow_messages = generate_messages_for_flow(SIMULATION_TIME)\n",
    "\n",
    "        for time, size in zip(*flow_messages):\n",
    "            tos = random.randint(0, PRIO_LEVELS-1)\n",
    "\n",
    "            messages.append({\n",
    "                'message_id': message_id,\n",
    "                'src': src,\n",
    "                'dst': dst,\n",
    "                'size': size,\n",
    "                'timestamp':time,\n",
    "                'tos': tos\n",
    "            })\n",
    "        \n",
    "        message_id += 1\n",
    "\n",
    "messages = sorted(messages, key=lambda x:x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 214/71791 [00:00<00:33, 2137.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71791/71791 [00:08<00:00, 8290.27it/s] \n"
     ]
    }
   ],
   "source": [
    "unique_packet_id = 0\n",
    "\n",
    "f =  open(out_name+'_'+parameters, 'w')\n",
    "\n",
    "for msg in tqdm(messages):\n",
    "    message_id, src, dst, size, timestamp, tos = msg.values()\n",
    "\n",
    "    num_packets = math.ceil(size / PACKET_SIZE) # ceil -> padding last packet to always be PACKET_SIZE\n",
    "    \n",
    "    packet_time = timestamp\n",
    "    packets = []\n",
    "    for _ in range(num_packets):\n",
    "        packet_info = (\n",
    "            f\"{str(unique_packet_id)} \"\n",
    "            f\"{str(message_id)} \"\n",
    "            f\"{str(src)} \"\n",
    "            f\"{str(dst)} \"\n",
    "            f\"{str(PACKET_SIZE)} \"\n",
    "            f\"{str(packet_time)} \"\n",
    "            f\"{str(tos)}\\n\"\n",
    "        )\n",
    "\n",
    "        f.write(packet_info)\n",
    "        \n",
    "        packet_time += PACKET_SIZE / BANDWIDTH\n",
    "\n",
    "        unique_packet_id += 1 \n",
    "\n",
    "f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
